{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detail about Legendre Polynomial Smoothing\n",
    "========\n",
    "\n",
    "For the curious who have good mathematics, basic information is supplied here, but this is not needed for the other sections.  It is a variation on the previous linear regression where we use the same linear functional form\n",
    "$$f(x) ~=~ \\sum_{i=0}^{I} a_i L_i(x)$$\n",
    "where $L_i(x)$ is the $i$-th Legendre polynomial (which is an $i$-order polynomial).\n",
    "Legendre polynomials are used instead of simple polynomials since they're a better basis set: simple powers have extreme values, and interact badly.\n",
    "\n",
    "Moreover, to fit $f(x)$ we minimise the augmented squared error\n",
    "$$\\frac{1}{\\sigma^2}\\sum_{i=1}^N (y_i-f(x_i))^2 + \\lambda {\\large\\int}_x (f''(x))^2 \\mbox{d}x$$\n",
    "where the second term involves the second derivative of $f()$ and induces *smoothness* in the resulting polynomial because it represents the mean square curvature. \n",
    "Bayesian theory is used to set the smoothing hyperparameter $\\lambda$.  The second term says, \"don't let the function $f()$ change too quickly.\"   The smoothing integral evaluates to be a sparse quadratic form in the parameters $(a_1,...,a_I)$. \n",
    "\n",
    "Discussion\n",
    "------\n",
    "\n",
    "Now is this a good general purpose fitting routine in 2-D?  Certainly not always.  In fact, its not even clear that such a thing as a \"general purpose routine\" exists.  Consider the following scenarios:\n",
    " * you're modelling exchange rate data at 5-minute intervals which can have wild changes;\n",
    " * you're modelling a fractal function, which means no matter what scale you fit, it seems somewhat the same,\n",
    " * you're modelling an industrial process known to undergo \"phase changes\" at different inputs, so occasional stark changes are expects.\n",
    " \n",
    "Moreover, the celibrated [\"no free lunch theorem\"](https://en.wikipedia.org/wiki/No_free_lunch_theorem) (NFLT) says, roughly,\n",
    "> if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems\n",
    " \n",
    "Clearly, our \"smoothing\" is not intrinsically useful in the contexts above, and indeed there most be other contexts where it cannot do as well too, by the NFLT.  You may see in some examples that does seem to make things a bit too smooth, for instance to try and smooth out peaks.\n",
    "\n",
    "Initialise\n",
    "---------\n",
    "\n",
    "First we reinitiallise things again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the pieces together, sin(x) + noise + basic regression \n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "sys.path.append(os.getcwd())\n",
    "import regressiondemo as rd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "rd.setSigma(0.2)\n",
    "\n",
    "#  don't make points more than 100 as demo is O(points^3)\n",
    "points = 30\n",
    "\n",
    "x = rd.makeX(points)\n",
    "\n",
    "# xts and yts store the \"true\" function for the purposes of plotting\n",
    "# these have to be high frequency to make the resultant plot look\n",
    "# like a smooth curve\n",
    "xts = rd.makeX(200,uniform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Basis Functions\n",
    "----------\n",
    "\n",
    "For those who want a glimpse of the gory details, read on.\n",
    "\n",
    "We plot below a few basis functions.  The first plot is the Legendre polynomials, which are rather like sin curves but modified so they fit into a finite range of [-1,1].  The final plot is the actual polynomial bases used by the regression routine.  Note the higher order bases have been scaled to be so small that they look flat, but they are actually very curvy, as in the second plot.  The effect of scaling is that in the final polynomials have no sharp deviations of curvature left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHOOSE:  degree of Legendre poly\n",
    "legdegree = 20\n",
    "legpoly = rd.LegPoly(legdegree)\n",
    "legpoly.setX(xts)\n",
    "vanders = legpoly.vander\n",
    "legpoly.setX(x)\n",
    "vander = legpoly.vander\n",
    "\n",
    "plist = [1,2,5,10,legdegree-1]\n",
    "\n",
    "# we will plot the first few Legendre polys\n",
    "for i in plist:\n",
    "    pl.plot(xts,vanders[:,i-1],label='Legendre_'+str(i))\n",
    "pl.legend(bbox_to_anchor=(1.4, 1.1))\n",
    "pl.suptitle('Legendre Polynomials of different order')\n",
    "pl.show()\n",
    "Vu, Vs, Vv = numpy.linalg.svd(legpoly.smooth)\n",
    "\n",
    "# we will plot the first few Smoothed Legendre polys before scaling\n",
    "for i in plist:\n",
    "    pl.plot(xts,numpy.dot(vanders,Vv[:,legdegree-i+1]),label='UnscSmthLgdre_'+str(i)) \n",
    "pl.legend(bbox_to_anchor=(1.6, 1.1))\n",
    "pl.suptitle('Smoothed Legendre Polynomials before scaling')\n",
    "pl.show()\n",
    "\n",
    "# now first few Smoothed Legendre polys with scaling\n",
    "for i in plist:\n",
    "    pl.plot(xts,numpy.dot(vanders,Vv[:,legdegree-i+1])/numpy.sqrt(Vs[legdegree-i+1]),label='SmoothLgdre_'+str(i)) \n",
    "pl.legend(bbox_to_anchor=(1.5, 1.1))\n",
    "pl.suptitle('Smoothed Legendre Polynomials')\n",
    "pl.show()\n",
    "\n",
    "# now plot random functions\n",
    "for i in range(10):\n",
    "    uu = numpy.random.normal(0,1,legdegree+1)\n",
    "    # uu[legdegree] = 0\n",
    "    pl.plot(xts,numpy.dot(vanders,numpy.dot(Vv,uu/numpy.sqrt(Vs)))) \n",
    "pl.suptitle('Random Smoothed Legendre Polys (centered, mean slope 0)')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "RegressionLegendreDetail.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
